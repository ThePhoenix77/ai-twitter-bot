AI Twitter Bot – Architecture Overview
=====================================

Core Modules
------------
1. **`config/config.py`** – Centralises runtime configuration such as niche keywords, the maximum number of articles to fetch, and the target number of tweets per batch.
2. **`fetcher.py`** – Uses `requests` and `NEWS_API_KEY` (from `.env`) to query NewsAPI, normalises each article into `{title, description, url}` dictionaries, and returns a list of results.
3. **`summarizer.py`** – Instantiates a Hugging Face `pipeline("summarization", model="facebook/bart-large-cnn")`, builds tweet-length summaries for each article, and scores them by keyword coverage via `score_summary`.
4. **`storage.py`** – Persists the generated tweets:
   - `data/daily_tweets.txt` holds the current batch generated by the latest run.
   - `data/tweets.txt` stores a chronological history used for duplicate detection.
5. **`tweeter.py`** – Wraps `tweety.TweetClient`, logs in with `TWITTER_USERNAME`/`TWITTER_PASSWORD`, and posts each new tweet while logging any failures.
6. **`main.py`** – Entry point that orchestrates the entire workflow and prints diagnostic information.

Data Flow
---------
```
		+----------------+
		| Manual trigger |
		+----------------+
				 |
				 v
			 main.py
				 |
				 v
	  fetcher.fetch_news()
				 |
				 v
   summarizer.summarize_article()
				 |
				 v
  storage.save_daily_tweets()
				 |
				 v
  storage.filter_duplicates()
				 |
				 v
	  tweeter.tweet_daily()
```

Execution Steps
---------------
1. Load environment variables (`python-dotenv`) and run `fetch_news` to obtain headlines relevant to the configured keywords.
2. Produce tweet-sized summaries via BART and rank them by keyword relevance, keeping the top `TWEET_COUNT` items.
3. Write the batch to disk, update the historical log, and filter out any summaries that have already been tweeted.
4. Log into X using `tweety` and post each remaining tweet sequentially.

Runtime Environment
-------------------
- **Language:** Python 3.11+
- **Dependencies:** `requests`, `python-dotenv`, `transformers`, `torch`, `sentencepiece`, `tweety-ns`.
- **State:** Flat files under `data/` for persistence; no external database is required.
- **Secrets:** `.env` file (ignored by git) contains API keys and X credentials.

Extensibility Notes
-------------------
- Introduce scheduling by wrapping `python main.py` in cron, GitHub Actions, or another job runner.
- Enhance duplicate detection with semantic similarity models if keyword checks prove insufficient.
- Add richer logging/alerting or migrate storage to a managed database if multi-instance hosting is required.
